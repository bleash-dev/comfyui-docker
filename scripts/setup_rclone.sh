#!/bin/bash
set -eo pipefail # Ensures script exits on error and handles pipe failures

echo "üîß Setting up rclone S3 mounting..."

# Validate that NETWORK_VOLUME was set by start.sh
if [ -z "$NETWORK_VOLUME" ]; then
    echo "‚ùå CRITICAL: NETWORK_VOLUME not set by start.sh. This script cannot proceed."
    exit 1
fi
echo "üìÅ Using Network Volume: $NETWORK_VOLUME"

# Create scripts directory on the network volume if it doesn't exist
# This is where scripts generated by create_*.sh might go.
mkdir -p "$NETWORK_VOLUME/scripts"
RCLONE_CACHE_DIR="$NETWORK_VOLUME/.cache/rclone" # Centralize rclone cache on persistent volume
mkdir -p "$RCLONE_CACHE_DIR"

# Validate required environment variables
required_vars=("AWS_BUCKET_NAME" "AWS_ACCESS_KEY_ID" "AWS_SECRET_ACCESS_KEY" "AWS_REGION" "POD_USER_NAME" "POD_ID")
for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        echo "‚ùå CRITICAL: Required environment variable $var is not set."
        if [ "$var" = "POD_ID" ]; then
            echo "POD_ID is required for pod-specific data isolation. Container startup ABORTED."
        fi
        exit 1 # Exit for any missing required variable
    fi
done

echo "‚úÖ Environment variables validated."
echo "   Bucket: $AWS_BUCKET_NAME, Region: $AWS_REGION, User: $POD_USER_NAME, Pod: $POD_ID"

# Create rclone configuration
RCLONE_CONFIG_DIR="/root/.config/rclone" # Standard location
RCLONE_CONFIG_FILE="$RCLONE_CONFIG_DIR/rclone.conf"
mkdir -p "$RCLONE_CONFIG_DIR"
echo "üìù Creating rclone configuration at $RCLONE_CONFIG_FILE..."
cat > "$RCLONE_CONFIG_FILE" << EOF
[s3]
type = s3
provider = AWS
access_key_id = $AWS_ACCESS_KEY_ID
secret_access_key = $AWS_SECRET_ACCESS_KEY
region = $AWS_REGION
acl = private
storage_class = STANDARD
# Optionally add endpoint_url if using a non-standard S3 endpoint or S3-compatible storage
# endpoint_url =
EOF
echo "‚úÖ Rclone configuration created."

# Test rclone connection
echo "üîç Testing S3 connection to bucket '$AWS_BUCKET_NAME'..."
if ! rclone lsd "s3:$AWS_BUCKET_NAME/" --retries 2 --max-depth 1 >/dev/null 2>&1; then
    echo "‚ùå CRITICAL: Failed to connect to S3 bucket '$AWS_BUCKET_NAME'."
    echo "   Please check AWS credentials, bucket name, region, and network connectivity."
    exit 1
fi
echo "‚úÖ S3 connection successful."

# Define folder structures for shared mounts
# These are locations on S3 under "shared/" that will be mounted read-only or with specific caching.
SHARED_FOLDERS=("venv" ".comfyui") # e.g., shared Python environments, base .comfyui configs
COMFYUI_SHARED_FOLDERS=("models" "custom_nodes") # e.g., shared ComfyUI models and custom nodes

# Create all sync and utility scripts FIRST before any operations that depend on them
# These scripts are assumed to be part of the container image at /scripts/
echo "üìù Creating/configuring dynamic scripts via image-based creator scripts..."
if [ -f /scripts/create_sync_scripts.sh ]; then
    if ! bash /scripts/create_sync_scripts.sh; then
        echo "‚ùå CRITICAL: Failed to create sync scripts via /scripts/create_sync_scripts.sh."
        exit 1
    fi
    echo "  ‚úÖ Sync scripts created/configured."
else
    echo "‚ö†Ô∏è Warning: /scripts/create_sync_scripts.sh not found. Assuming not needed or handled elsewhere."
fi

if [ -f /scripts/create_monitoring_scripts.sh ]; then
    if ! bash /scripts/create_monitoring_scripts.sh; then
        echo "‚ùå CRITICAL: Failed to create monitoring scripts via /scripts/create_monitoring_scripts.sh."
        exit 1
    fi
    echo "  ‚úÖ Monitoring scripts created/configured."
else
    echo "‚ö†Ô∏è Warning: /scripts/create_monitoring_scripts.sh not found."
fi

if [ -f /scripts/create_utility_scripts.sh ]; then
    if ! bash /scripts/create_utility_scripts.sh; then
        echo "‚ùå CRITICAL: Failed to create utility scripts via /scripts/create_utility_scripts.sh."
        exit 1
    fi
    echo "  ‚úÖ Utility scripts created/configured."
else
    echo "‚ö†Ô∏è Warning: /scripts/create_utility_scripts.sh not found."
fi
echo "‚úÖ Dynamic script creation/configuration phase completed."


# Mount shared folders from S3
echo "üîó Setting up shared folder mounts from S3..."
mount_failures=()

# Function to perform mount with retry and validation
mount_with_validation() {
    local s3_path="$1"
    local mount_point="$2"
    local folder_name="$3" # For logging purposes
    
    echo "  Attempting to mount '$folder_name': $s3_path -> $mount_point"
    mkdir -p "$mount_point" # Ensure local mount point directory exists
    
    # Check if the S3 path actually exists and has content or is a "directory"
    if rclone lsd "$s3_path" --retries 1 --max-depth 1 >/dev/null 2>&1; then # Check if it's a listable directory
        echo "    S3 source '$s3_path' found. Proceeding with mount for '$folder_name'."
        
        # Consider adding --read-only if these shared folders should not be modified by the pod
        # Add --poll-interval if changes on S3 need to be detected faster than dir-cache-time
        rclone mount "$s3_path" "$mount_point" \
            --daemon \
            --allow-other \
            --allow-non-empty \
            --dir-cache-time 5m \
            --poll-interval 1m \
            --vfs-cache-mode full \
            --vfs-cache-max-age 7d \
            --vfs-cache-max-size 5G \
            --vfs-read-chunk-size 64M \
            --vfs-read-chunk-size-limit off \
            --buffer-size 64M \
            --timeout 5m \
            --retries 3 \
            --cache-dir "$RCLONE_CACHE_DIR/vfs/$POD_ID/$(basename "$mount_point")" \
            --log-level INFO \
            --log-file "$NETWORK_VOLUME/.logs/rclone_mount_$(echo "$folder_name" | tr '/' '_').log" # Per-mount log file
            # Add --read-only if applicable

        # Wait a bit for the mount daemon to stabilize and then validate
        sleep 5 # Giving rclone daemon a few seconds
        
        if mountpoint -q "$mount_point"; then
            echo "    '$mount_point' is a mountpoint. Verifying access..."
            if timeout 15 ls -A "$mount_point" >/dev/null 2>&1; then # Check if we can list (non-empty check)
                echo "    ‚úÖ Successfully mounted and verified '$folder_name' at '$mount_point'."
                return 0 # Success
            else
                echo "    ‚ùå ERROR: Mounted '$folder_name' at '$mount_point', but failed to list contents (timeout or permission issue)."
                # Attempt to unmount the failed mount
                fusermount -uz "$mount_point" 2>/dev/null || umount -l "$mount_point" 2>/dev/null || true
                return 1 # Failure
            fi
        else
            echo "    ‚ùå ERROR: Failed to mount '$folder_name'. '$mount_point' is not a mountpoint after rclone command."
            return 1 # Failure
        fi
    else
        echo "    ‚ö†Ô∏è WARNING: S3 source path '$s3_path' for '$folder_name' not found or not listable."
        echo "               Creating an empty local directory at '$mount_point' as a placeholder."
        # mkdir -p "$mount_point" is already done above.
        # Decide if this is an error or acceptable. For shared resources, it might be an error.
        # For now, let's treat missing shared S3 folders as a potential problem that might be critical.
        echo "    Consider this a failure if '$folder_name' data was expected from S3."
        return 1 # Treat missing S3 source for shared data as a failure.
                 # Change to 'return 0' if an empty local placeholder is acceptable.
    fi
}

# Ensure log directory for rclone mounts exists
mkdir -p "$NETWORK_VOLUME/.logs"

# Mount main shared folders
for folder in "${SHARED_FOLDERS[@]}"; do
    mount_point="$NETWORK_VOLUME/$folder"
    s3_path="s3:$AWS_BUCKET_NAME/pod_sessions/shared/$folder/" # Ensure trailing slash for S3 "directory"
    
    if ! mount_with_validation "$s3_path" "$mount_point" "shared/$folder"; then
        mount_failures+=("shared/$folder")
    fi
done

# Mount ComfyUI shared subfolders
# Ensure local ComfyUI base directory exists for these mounts
mkdir -p "$NETWORK_VOLUME/ComfyUI"
for folder in "${COMFYUI_SHARED_FOLDERS[@]}"; do
    mount_point="$NETWORK_VOLUME/ComfyUI/$folder"
    s3_path="s3:$AWS_BUCKET_NAME/pod_sessions/shared/ComfyUI/$folder/" # Ensure trailing slash
    
    if ! mount_with_validation "$s3_path" "$mount_point" "ComfyUI-shared/$folder"; then
        mount_failures+=("ComfyUI-shared/$folder")
    fi
done

# Check for critical mount failures
if [[ ${#mount_failures[@]} -gt 0 ]]; then
    echo "‚ùå CRITICAL ERROR: Failed to mount one or more S3 shared folders: ${mount_failures[*]}"
    echo "   Essential shared data may be missing. Startup ABORTED."
    # Consider attempting to unmount any successful mounts before exiting
    # for mount_dir in "$NETWORK_VOLUME/"* "$NETWORK_VOLUME/ComfyUI/"*; do
    #   if mountpoint -q "$mount_dir"; then fusermount -uz "$mount_dir" 2>/dev/null || umount -l "$mount_dir" 2>/dev/null; fi
    # done
    exit 1
fi
echo "‚úÖ All shared S3 folders mounted successfully."


# Sync user-specific data from S3
# Assuming sync_user_data_from_s3.sh is now at $NETWORK_VOLUME/scripts/ or /scripts/
# Clarify this path based on where create_sync_scripts.sh places it.
USER_SYNC_SCRIPT_PATH=""
if [ -f "$NETWORK_VOLUME/scripts/sync_user_data_from_s3.sh" ]; then
    USER_SYNC_SCRIPT_PATH="$NETWORK_VOLUME/scripts/sync_user_data_from_s3.sh"
elif [ -f "/scripts/sync_user_data_from_s3.sh" ]; then
    USER_SYNC_SCRIPT_PATH="/scripts/sync_user_data_from_s3.sh"
fi

if [ -n "$USER_SYNC_SCRIPT_PATH" ]; then
    echo "üë§ Syncing user-specific data from S3 via $USER_SYNC_SCRIPT_PATH..."
    if ! bash "$USER_SYNC_SCRIPT_PATH"; then # This script should handle its own error reporting
        echo "‚ö†Ô∏è WARNING: User-specific data sync encountered issues. Startup will continue."
        # Decide if this should be a critical failure. Usually, user data sync might be less critical than shared mounts.
    fi
    echo "‚úÖ User-specific data sync process completed."
else
    echo "‚ö†Ô∏è WARNING: User data sync script (sync_user_data_from_s3.sh) not found in expected locations. Skipping user data sync."
fi

echo "‚úÖ Rclone S3 setup and initial syncs completed successfully!"