name: Build and Deploy ComfyUI AMI

on:
  push:
    branches: [ main, dev]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1

jobs:
  create-ami:
    runs-on: ubuntu-latest
    environment: "prod"
    if: github.event_name != 'pull_request'

    permissions:
      id-token: write
      contents: read

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Configure AWS credentials via OIDC
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: ${{ secrets.GH_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Get base AMI (existing ComfyUI AMI or Ubuntu)
      id: base-ami
      run: |
        ENVIRONMENT="${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}"
        echo "üîç Looking for existing ComfyUI AMI for environment: $ENVIRONMENT"
        
        # First, try to get the latest ComfyUI AMI for this environment
        # Try multiple search patterns to catch different naming conventions
        EXISTING_AMI=""
        
        # Pattern 1: Exact name match (comfyui-multitenant-dev)
        EXISTING_AMI=$(aws ec2 describe-images \
          --owners self \
          --filters "Name=name,Values=comfyui-multitenant-$ENVIRONMENT" \
          --query 'Images[0].ImageId' \
          --output text \
          --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
        
        # Pattern 2: Name with wildcard (comfyui-multitenant-dev-*)
        if [ -z "$EXISTING_AMI" ] || [ "$EXISTING_AMI" = "None" ]; then
          EXISTING_AMI=$(aws ec2 describe-images \
            --owners self \
            --filters "Name=name,Values=comfyui-multitenant-$ENVIRONMENT-*" \
            --query 'Images | sort_by(@, &CreationDate) | [-1].ImageId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
        fi
        
        # Pattern 3: Tag-based search for Environment
        if [ -z "$EXISTING_AMI" ] || [ "$EXISTING_AMI" = "None" ]; then
          EXISTING_AMI=$(aws ec2 describe-images \
            --owners self \
            --filters "Name=tag:Environment,Values=$ENVIRONMENT" \
            --query 'Images | sort_by(@, &CreationDate) | [-1].ImageId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
        fi
        
        if [ -n "$EXISTING_AMI" ] && [ "$EXISTING_AMI" != "None" ]; then
          echo "‚úÖ Found existing ComfyUI AMI: $EXISTING_AMI"
          echo "üîÑ Will update existing AMI instead of creating from scratch"
          echo "ami-id=$EXISTING_AMI" >> $GITHUB_OUTPUT
          echo "ami-type=existing" >> $GITHUB_OUTPUT
          
          # Get existing AMI details for reference
          aws ec2 describe-images \
            --image-ids $EXISTING_AMI \
            --query 'Images[0].[Name,Description,CreationDate]' \
            --output table \
            --region ${{ env.AWS_REGION }}
        else
          echo "üìù No existing ComfyUI AMI found, using fresh Ubuntu base"
          # Get latest Ubuntu AMI
          UBUNTU_AMI=$(aws ec2 describe-images \
            --owners 099720109477 \
            --filters "Name=name,Values=ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*" \
                "Name=state,Values=available" \
            --query "Images | sort_by(@, &CreationDate) | [-1].ImageId" \
            --output text \
            --region ${{ env.AWS_REGION }})
          
          echo "üñ•Ô∏è Using base Ubuntu AMI: $UBUNTU_AMI"
          echo "ami-id=$UBUNTU_AMI" >> $GITHUB_OUTPUT
          echo "ami-type=ubuntu" >> $GITHUB_OUTPUT
        fi

    - name: Upload scripts to S3
      run: |
        ENVIRONMENT="${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}"
        S3_PREFIX="s3://viral-comm-api-ec2-deployments-dev/comfy-docker/${ENVIRONMENT}"
        
        echo "üì§ Uploading all required scripts to S3..."
        echo "S3 path: ${S3_PREFIX}"
        
        # Upload all preparation and management scripts
        aws s3 cp scripts/prepare_ami.sh "${S3_PREFIX}/prepare_ami.sh" --region ${{ env.AWS_REGION }}
        aws s3 cp scripts/setup_cloudwatch.sh "${S3_PREFIX}/setup_cloudwatch.sh" --region ${{ env.AWS_REGION }}
        aws s3 cp scripts/tenant_manager.py "${S3_PREFIX}/tenant_manager.py" --region ${{ env.AWS_REGION }}
        aws s3 cp scripts/create_s3_interactor.sh "${S3_PREFIX}/create_s3_interactor.sh" --region ${{ env.AWS_REGION }}
        
        # Upload any other scripts that exist
        find scripts/ -name "*.sh" -not -path "scripts/prepare_ami.sh" -not -path "scripts/setup_cloudwatch.sh" -not -path "scripts/create_s3_interactor.sh" -exec aws s3 cp {} "${S3_PREFIX}/$(basename {})" --region ${{ env.AWS_REGION }} \;
        find scripts/ -name "*.py" -not -path "scripts/tenant_manager.py" -exec aws s3 cp {} "${S3_PREFIX}/$(basename {})" --region ${{ env.AWS_REGION }} \;
        
        echo "‚úÖ All scripts uploaded to S3"
        echo "s3-prefix=${S3_PREFIX}" >> $GITHUB_OUTPUT

    - name: Launch EC2 instance for AMI creation
      id: launch-instance
      timeout-minutes: 5
      run: |
        ENVIRONMENT="${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}"
        INSTANCE_NAME="comfyui-ami-builder-${ENVIRONMENT}-$(date +%Y%m%d-%H%M%S)"
        
        echo "üöÄ Launching EC2 instance for AMI creation..."
        echo "Environment: $ENVIRONMENT"
        echo "Instance name: $INSTANCE_NAME"
        
        # Create user data script
        ENVIRONMENT="${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}"
        S3_PREFIX="s3://viral-comm-api-ec2-deployments-dev/comfy-docker/${ENVIRONMENT}"
        AMI_TYPE="${{ steps.base-ami.outputs.ami-type }}"
        
        cat > user-data.sh << EOF
        #!/bin/bash
        set -e
        
        # Set environment variables
        export DEBIAN_FRONTEND=noninteractive
        export AMI_TYPE="$AMI_TYPE"
        export ENVIRONMENT="$ENVIRONMENT"
        export S3_PREFIX="$S3_PREFIX"
        
        # Log everything
        exec > >(tee /var/log/user-data.log) 2>&1
        echo "Starting AMI update at \$(date)"
        echo "AMI Type: \$AMI_TYPE"
        
        if [ "\$AMI_TYPE" = "existing" ]; then
          echo "üîÑ Updating existing ComfyUI AMI (Docker-Free)"
          echo "UPDATING_EXISTING_AMI" > /tmp/ami_progress.txt
          
          # Stop the tenant manager service for clean AMI state
          echo "ÔøΩ Stopping ComfyUI services for clean AMI state..."
          systemctl stop comfyui-multitenant.service || true
          
          # Clean up any running ComfyUI processes
          echo "üßπ Cleaning up any running ComfyUI processes..."
          pkill -f "python.*ComfyUI" || true
          pkill -f "python.*tenant_manager" || true
          
          # Download and update scripts from S3
          echo "üì• Downloading updated scripts from S3..."
          mkdir -p /tmp/scripts
          cd /tmp/scripts
          
          # Download all scripts
          aws s3 cp "${S3_PREFIX}/tenant_manager.py" tenant_manager.py --region ${{ env.AWS_REGION }}
          aws s3 cp "${S3_PREFIX}/setup_cloudwatch.sh" setup_cloudwatch.sh --region ${{ env.AWS_REGION }}
          aws s3 cp "${S3_PREFIX}/create_s3_interactor.sh" create_s3_interactor.sh --region ${{ env.AWS_REGION }}
          
          # Update tenant manager
          cp tenant_manager.py /usr/local/bin/tenant_manager.py
          chmod +x /usr/local/bin/tenant_manager.py
          
          # Update other scripts
          cp setup_cloudwatch.sh /scripts/setup_cloudwatch.sh || mkdir -p /scripts && cp setup_cloudwatch.sh /scripts/
          cp create_s3_interactor.sh /scripts/create_s3_interactor.sh
          chmod +x /scripts/*.sh
          
          echo "‚úÖ Scripts updated successfully"
          echo "EXISTING_AMI_UPDATED" > /tmp/ami_progress.txt
          
        else
          echo "üÜï Setting up fresh ComfyUI AMI from Ubuntu base (Docker-Free)"
          echo "FRESH_SETUP_STARTING" > /tmp/ami_progress.txt
          
          # Update system
          apt-get update -y
          
          # Install required packages
          apt-get install -y awscli curl wget jq
          
          # Download scripts from S3
          echo "üì• Downloading setup scripts from S3..."
          mkdir -p /scripts
          cd /scripts
          
          # Download all required scripts
          aws s3 cp "${S3_PREFIX}/prepare_ami.sh" prepare_ami.sh --region ${{ env.AWS_REGION }}
          aws s3 cp "${S3_PREFIX}/setup_cloudwatch.sh" setup_cloudwatch.sh --region ${{ env.AWS_REGION }}
          aws s3 cp "${S3_PREFIX}/tenant_manager.py" tenant_manager.py --region ${{ env.AWS_REGION }}
          aws s3 cp "${S3_PREFIX}/create_s3_interactor.sh" create_s3_interactor.sh --region ${{ env.AWS_REGION }}
          
          # Make all scripts executable
          chmod +x /scripts/*.sh
          chmod +x /scripts/*.py
          
          # Run the full AMI preparation (Docker-free)
          echo "üîß Running full AMI preparation (Docker-Free)..."
          echo "RUNNING_FULL_SETUP" > /tmp/ami_progress.txt
          
          # Run with timeout to prevent infinite hangs
          timeout 1800 /scripts/prepare_ami.sh || {
              echo "‚ùå AMI preparation script timed out or failed"
              echo "Exit code: \$?"
              echo "Last checkpoint:"
              cat /tmp/ami_progress.txt 2>/dev/null || echo "No progress file found"
              exit 1
          }
          
          echo "FRESH_SETUP_COMPLETED" > /tmp/ami_progress.txt
        fi
        
        # Common final steps for both scenarios
        echo "üîç Verifying system setup..."
        if ! command -v python3 >/dev/null; then
          echo "‚ùå Python3 not found after setup"
          exit 1
        fi
        
        if [ ! -f "/usr/local/bin/tenant_manager.py" ]; then
          echo "‚ùå Tenant manager not found after setup"
          exit 1
        fi
        
        echo "üßπ Final cleanup..."
        # Clean up temporary files
        rm -rf /tmp/scripts || true
        
        # Signal completion
        echo "AMI_UPDATE_COMPLETE" > /tmp/ami_ready.txt
        echo "AMI update completed at \$(date)"
        EOF
        
        # Get VPC and subnet with specific preference
        echo "üîç Finding VPC and subnet (preferred: viral-comm-api-proxy-vpc-dev)..."
        
        # Try to find the specific VPC first
        PREFERRED_VPC_NAME="viral-comm-api-viral-community-stack-dev/viral-comm-api-common-networking-dev/viral-comm-api-proxy-vpc-dev"
        PREFERRED_SUBNET_NAME="*vpc-proxy-subnetSubnet1*"
        PREFERRED_SUBNET_LOGICAL_ID="vpc-proxy-subnetSubnet1"
        
        echo "üéØ Looking for preferred VPC: $PREFERRED_VPC_NAME"
        
        # Try to find VPC by Name tag (CDK-managed resources usually have Name tags)
        VPC_ID=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Name,Values=$PREFERRED_VPC_NAME" \
          --query 'Vpcs[0].VpcId' \
          --output text \
          --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
        
        # Also try shorter name patterns in case the full path isn't used
        if [[ -z "$VPC_ID" || "$VPC_ID" == "None" ]]; then
          echo "üîç Trying shorter VPC name pattern: *proxy-vpc-dev"
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*proxy-vpc-dev" \
            --query 'Vpcs[0].VpcId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
        fi
        
        # Fallback to default VPC
        if [[ -z "$VPC_ID" || "$VPC_ID" == "None" ]]; then
          echo "‚ö†Ô∏è Preferred VPC not found, trying default VPC..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=is-default,Values=true" \
            --query 'Vpcs[0].VpcId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
        fi
        
        # Final fallback to any available VPC
        if [[ -z "$VPC_ID" || "$VPC_ID" == "None" ]]; then
          echo "‚ö†Ô∏è No default VPC found, using any available VPC..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --query 'Vpcs[0].VpcId' \
            --output text \
            --region ${{ env.AWS_REGION }})
        fi
        
        echo "‚úÖ Selected VPC: $VPC_ID"
        
        # Now find the subnet - try preferred subnet first
        if [[ "$VPC_ID" != "None" && -n "$VPC_ID" ]]; then
          echo "üéØ Looking for preferred subnet: vpc-proxy-subnetSubnet1"
          
          SUBNET_ID=$(aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=${VPC_ID}" "Name=tag:Name,Values=$PREFERRED_SUBNET_NAME" \
            --query 'Subnets[0].SubnetId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          
          # Try CDK logical ID pattern
          if [[ -z "$SUBNET_ID" || "$SUBNET_ID" == "None" ]]; then
            echo "üîç Trying CDK logical ID pattern: *vpc-proxy-subnetSubnet1*"
            SUBNET_ID=$(aws ec2 describe-subnets \
              --filters "Name=vpc-id,Values=${VPC_ID}" "Name=tag:aws:cloudformation:logical-id,Values=*vpc-proxy-subnetSubnet1*" \
              --query 'Subnets[0].SubnetId' \
              --output text \
              --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          fi
          
          # Try looking for any SubnetSubnet1 pattern
          if [[ -z "$SUBNET_ID" || "$SUBNET_ID" == "None" ]]; then
            echo "üîç Trying generic SubnetSubnet1 pattern: *SubnetSubnet1*"
            SUBNET_ID=$(aws ec2 describe-subnets \
              --filters "Name=vpc-id,Values=${VPC_ID}" "Name=tag:Name,Values=*SubnetSubnet1*" \
              --query 'Subnets[0].SubnetId' \
              --output text \
              --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          fi
          
          # Fallback to any public subnet in the VPC
          if [[ -z "$SUBNET_ID" || "$SUBNET_ID" == "None" ]]; then
            echo "‚ö†Ô∏è Preferred subnet not found, looking for any public subnet in VPC..."
            SUBNET_ID=$(aws ec2 describe-subnets \
              --filters "Name=vpc-id,Values=${VPC_ID}" "Name=map-public-ip-on-launch,Values=true" \
              --query 'Subnets[0].SubnetId' \
              --output text \
              --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          fi
          
          # Final fallback to any subnet in the VPC
          if [[ -z "$SUBNET_ID" || "$SUBNET_ID" == "None" ]]; then
            echo "‚ö†Ô∏è No public subnet found, using any available subnet in VPC..."
            SUBNET_ID=$(aws ec2 describe-subnets \
              --filters "Name=vpc-id,Values=${VPC_ID}" \
              --query 'Subnets[0].SubnetId' \
              --output text \
              --region ${{ env.AWS_REGION }})
          fi
        fi
        
        echo "‚úÖ Selected Subnet: $SUBNET_ID"
        
        # Verify we have valid VPC and subnet
        if [[ -z "$VPC_ID" || "$VPC_ID" == "None" || -z "$SUBNET_ID" || "$SUBNET_ID" == "None" ]]; then
          echo "‚ùå Could not find valid VPC and subnet combination"
          echo "VPC_ID: $VPC_ID"
          echo "SUBNET_ID: $SUBNET_ID"
          exit 1
        fi
        
        # Get VPC and subnet names for better logging
        VPC_NAME=$(aws ec2 describe-vpcs \
          --vpc-ids "$VPC_ID" \
          --query 'Vpcs[0].Tags[?Key==`Name`].Value' \
          --output text \
          --region ${{ env.AWS_REGION }} 2>/dev/null || echo "No name tag")
        
        SUBNET_NAME=$(aws ec2 describe-subnets \
          --subnet-ids "$SUBNET_ID" \
          --query 'Subnets[0].Tags[?Key==`Name`].Value' \
          --output text \
          --region ${{ env.AWS_REGION }} 2>/dev/null || echo "No name tag")
        
        # Log what we're using with better details
        echo "üìã Network Configuration Summary:"
        echo "   VPC ID: $VPC_ID (Name: $VPC_NAME)"
        echo "   Subnet ID: $SUBNET_ID (Name: $SUBNET_NAME)"
        
        # Check if we got our preferred resources
        if [[ "$VPC_NAME" == *"viral-comm-api-proxy-vpc-dev"* ]]; then
          echo "üéâ Successfully using preferred viral-comm-api VPC!"
        else
          echo "‚ö†Ô∏è Using fallback VPC - preferred VPC not found"
        fi
        
        if [[ "$SUBNET_NAME" == *"vpc-proxy-subnetSubnet1"* ]]; then
          echo "üéâ Successfully using preferred subnet!"
        else
          echo "‚ö†Ô∏è Using fallback subnet - preferred subnet not found"
        fi
        
        # Create IAM role for EC2 instance to access S3
        echo "üîê Creating IAM role for EC2 instance..."
        ROLE_NAME="comfyui-ami-builder-role-$(date +%s)"
        
        # Create trust policy
        cat > trust-policy.json << 'TRUST_EOF'
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "Service": "ec2.amazonaws.com"
              },
              "Action": "sts:AssumeRole"
            }
          ]
        }
        TRUST_EOF
        
        # Create IAM role
        aws iam create-role \
          --role-name "$ROLE_NAME" \
          --assume-role-policy-document file://trust-policy.json \
          --region ${{ env.AWS_REGION }}
        
        # Attach S3 read-only policy
        aws iam attach-role-policy \
          --role-name "$ROLE_NAME" \
          --policy-arn "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess" \
          --region ${{ env.AWS_REGION }}
        
        # Attach SSM policy for status checking
        aws iam attach-role-policy \
          --role-name "$ROLE_NAME" \
          --policy-arn "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore" \
          --region ${{ env.AWS_REGION }}
        
        # Attach CloudWatch logs policy for debugging
        aws iam attach-role-policy \
          --role-name "$ROLE_NAME" \
          --policy-arn "arn:aws:iam::aws:policy/CloudWatchLogsFullAccess" \
          --region ${{ env.AWS_REGION }}
        
        # Create instance profile
        aws iam create-instance-profile \
          --instance-profile-name "$ROLE_NAME" \
          --region ${{ env.AWS_REGION }}
        
        # Add role to instance profile
        aws iam add-role-to-instance-profile \
          --instance-profile-name "$ROLE_NAME" \
          --role-name "$ROLE_NAME" \
          --region ${{ env.AWS_REGION }}
        
        # Wait a moment for IAM to propagate
        sleep 15
        
        echo "‚úÖ IAM role created: $ROLE_NAME"
        
        # Find security group - prefer CDK-managed security group from viral-comm-api stack
        echo "üîí Finding security group (preferred: viral-comm-api proxy security group)..."
        
        PREFERRED_SG_NAME="viral-comm-api-proxy-security-dev"
        
        # Try to find the CDK-managed security group first
        SG_ID=$(aws ec2 describe-security-groups \
          --filters "Name=vpc-id,Values=$VPC_ID" "Name=tag:Name,Values=$PREFERRED_SG_NAME" \
          --query 'SecurityGroups[0].GroupId' \
          --output text \
          --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
        
        # Try shorter patterns for the security group
        if [[ -z "$SG_ID" || "$SG_ID" == "None" ]]; then
          echo "üîç Trying shorter security group name pattern: *proxy-security*"
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" "Name=tag:Name,Values=*proxy-security*" \
            --query 'SecurityGroups[0].GroupId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
        fi
        
        # Try group name instead of tag
        if [[ -z "$SG_ID" || "$SG_ID" == "None" ]]; then
          echo "üîç Trying security group by group name: *proxy-security*"
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" "Name=group-name,Values=*proxy-security*" \
            --query 'SecurityGroups[0].GroupId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
        fi
        
        # Fallback to default security group
        if [[ -z "$SG_ID" || "$SG_ID" == "None" ]]; then
          echo "‚ö†Ô∏è Preferred security group not found, using default security group..."
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" "Name=group-name,Values=default" \
            --query 'SecurityGroups[0].GroupId' \
            --output text \
            --region ${{ env.AWS_REGION }})
        fi
        
        echo "‚úÖ Selected Security Group: $SG_ID"
        
        # Log what security group we're using and its rules
        SG_NAME=$(aws ec2 describe-security-groups \
          --group-ids "$SG_ID" \
          --query 'SecurityGroups[0].GroupName' \
          --output text \
          --region ${{ env.AWS_REGION }} 2>/dev/null || echo "unknown")
        
        if [[ "$SG_NAME" == *"proxy-security"* ]]; then
          echo "üéâ Using viral-comm-api proxy security group with proper ComfyUI ports"
        else
          echo "‚ö†Ô∏è Using fallback security group: $SG_NAME"
          echo "‚ö†Ô∏è Note: This may not have the required ports open for ComfyUI access"
        fi
        
        # Display security group rules for debugging
        echo "üìã Security Group Rules:"
        aws ec2 describe-security-groups \
          --group-ids "$SG_ID" \
          --query 'SecurityGroups[0].IpPermissions[*].[IpProtocol,FromPort,ToPort,IpRanges[*].CidrIp]' \
          --output table \
          --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Could not retrieve security group rules"
        
        # Launch instance with VPC configuration and IAM role
        INSTANCE_ID=$(aws ec2 run-instances \
          --image-id ${{ steps.base-ami.outputs.ami-id }} \
          --instance-type t3.large \
          --subnet-id $SUBNET_ID \
          --associate-public-ip-address \
          --iam-instance-profile Name="$ROLE_NAME" \
          --user-data file://user-data.sh \
          --region ${{ env.AWS_REGION }} \
          --tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=${INSTANCE_NAME}},{Key=Environment,Value=${ENVIRONMENT}},{Key=Purpose,Value=AMI-Builder},{Key=AutoTerminate,Value=true},{Key=IAMRole,Value=${ROLE_NAME}}]" \
          --query 'Instances[0].InstanceId' \
          --output text)
        
        echo "instance-id=$INSTANCE_ID" >> $GITHUB_OUTPUT
        echo "iam-role=$ROLE_NAME" >> $GITHUB_OUTPUT
        
        # Get the public IP for network testing
        PUBLIC_IP=$(aws ec2 describe-instances \
          --instance-ids $INSTANCE_ID \
          --region ${{ env.AWS_REGION }} \
          --query 'Reservations[0].Instances[0].PublicIpAddress' \
          --output text)
        
        echo "public-ip=$PUBLIC_IP" >> $GITHUB_OUTPUT
        echo "üöÄ Launched instance: $INSTANCE_ID"
        echo "üåê Public IP: $PUBLIC_IP"
        echo "‚ö†Ô∏è Instance will be automatically terminated after AMI creation"
        echo "üí° Using VPC: $VPC_ID, Subnet: $SUBNET_ID, IAM Role: $ROLE_NAME"

    - name: Wait for instance setup completion
      timeout-minutes: 10
      run: |
        INSTANCE_ID="${{ steps.launch-instance.outputs.instance-id }}"
        echo "‚è≥ Waiting for instance setup to complete..."
        
        # Wait for instance to be running
        echo "üîÑ Waiting for instance to be running..."
        aws ec2 wait instance-running --instance-ids $INSTANCE_ID --region ${{ env.AWS_REGION }}
        echo "‚úÖ Instance is running"
        
        # Wait for SSM agent to be ready
        echo "üîÑ Waiting for SSM agent to be ready..."
        for i in {1..10}; do
          if aws ssm describe-instance-information \
            --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
            --region ${{ env.AWS_REGION }} \
            --query 'InstanceInformationList[0].PingStatus' \
            --output text 2>/dev/null | grep -q "Online"; then
            echo "‚úÖ SSM agent is online"
            break
          fi
          echo "‚è≥ SSM agent not ready yet, waiting... (attempt $i/10)"
          sleep 30
        done

    - name: Wait for AMI update completion
      timeout-minutes: 10
      run: |
        INSTANCE_ID="${{ steps.launch-instance.outputs.instance-id }}"
        AMI_TYPE="${{ steps.base-ami.outputs.ami-type }}"
        
        # Adjust monitoring based on AMI type
        if [ "$AMI_TYPE" = "existing" ]; then
          MAX_ATTEMPTS=20  # Existing AMI updates should be fast (5-10 minutes max)
          echo "üîß Monitoring AMI update progress (fast track for existing AMI)..."
        else
          MAX_ATTEMPTS=50  # Fresh setup takes longer (15-25 minutes)
          echo "üîß Monitoring AMI setup progress (full setup from Ubuntu)..."
        fi
        
        echo "AMI Type: $AMI_TYPE"
        echo "Max attempts: $MAX_ATTEMPTS"
        
        for i in $(seq 1 $MAX_ATTEMPTS); do
          echo "üîç Checking AMI update status... (attempt $i/$MAX_ATTEMPTS)"
          
          # Check current progress
          PROGRESS_CHECK_ID=$(aws ssm send-command \
            --instance-ids $INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=[
              "echo === Progress Check ===",
              "if [ -f /tmp/ami_progress.txt ]; then",
              "  echo CURRENT_PROGRESS: $(cat /tmp/ami_progress.txt)",
              "else",
              "  echo CURRENT_PROGRESS: UNKNOWN",
              "fi",
              "echo === Completion Check ===",
              "if [ -f /tmp/ami_ready.txt ]; then",
              "  echo FILE_EXISTS: READY",
              "  echo FILE_CONTENT: $(cat /tmp/ami_ready.txt)",
              "else",
              "  echo FILE_EXISTS: NOT_READY",
              "fi",
              "echo === Process Check ===",
              "ps aux | grep -E \"prepare_ami|user-data\" | grep -v grep || echo NO_PROCESS_RUNNING",
              "echo === Error Check ===",
              "tail -10 /var/log/user-data.log 2>/dev/null | grep -i error || echo NO_RECENT_ERRORS"
            ]' \
            --region ${{ env.AWS_REGION }} \
            --query 'Command.CommandId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$PROGRESS_CHECK_ID" ]; then
            sleep 10
            
            RESULT=$(aws ssm get-command-invocation \
              --command-id $PROGRESS_CHECK_ID \
              --instance-id $INSTANCE_ID \
              --region ${{ env.AWS_REGION }} \
              --query 'StandardOutputContent' \
              --output text 2>/dev/null || echo "PENDING")
            
            echo "Progress Status:"
            echo "$RESULT"
            
            # Check for errors in the logs
            if [[ "$RESULT" == *"ERROR:"* || "$RESULT" == *"FAILED"* || "$RESULT" == *"error:"* ]]; then
              echo "‚ùå Detected errors in AMI update logs"
              echo "üîç Getting detailed error information..."
              
              # Get detailed logs immediately when error is detected
              ERROR_LOG_ID=$(aws ssm send-command \
                --instance-ids $INSTANCE_ID \
                --document-name "AWS-RunShellScript" \
                --parameters 'commands=[
                  "echo === DETAILED ERROR INVESTIGATION ===",
                  "echo === User Data Log (last 100 lines) ===",
                  "tail -100 /var/log/user-data.log 2>/dev/null || echo No user-data log found",
                  "echo === AMI Preparation Log ===",
                  "tail -50 /var/log/ami-preparation.log 2>/dev/null || echo No AMI preparation log found",
                  "echo === CloudInit Output Log ===",
                  "tail -50 /var/log/cloud-init-output.log 2>/dev/null || echo No cloud-init-output log found",
                  "echo === Process Status ===",
                  "ps aux | grep -E \"prepare_ami|user-data\" || echo No relevant processes found",
                  "echo === Script Files Check ===",
                  "ls -la /scripts/ 2>/dev/null || echo Scripts directory not found",
                  "echo === Environment Variables (from user-data context) ===",
                  "env | grep -E \"DOCKER|COMFY|AMI\" || echo No relevant environment variables found",
                  "echo === System Status ===",
                  "df -h && echo && free -h"
                ]' \
                --region ${{ env.AWS_REGION }} \
                --query 'Command.CommandId' \
                --output text 2>/dev/null || echo "")
              
              if [ -n "$ERROR_LOG_ID" ]; then
                sleep 10
                ERROR_DETAILS=$(aws ssm get-command-invocation \
                  --command-id $ERROR_LOG_ID \
                  --instance-id $INSTANCE_ID \
                  --region ${{ env.AWS_REGION }} \
                  --query 'StandardOutputContent' \
                  --output text 2>/dev/null || echo "Could not retrieve error details")
                
                echo "=== DETAILED ERROR INFORMATION ==="
                echo "$ERROR_DETAILS"
                echo "================================="
              fi
              
              exit 1
            fi
            
            # Check for completion - either file exists (READY) or contains completion marker
            if [[ "$RESULT" == *"FILE_EXISTS: READY"* ]] || [[ "$RESULT" == *"AMI_UPDATE_COMPLETE"* ]]; then
              echo "‚úÖ AMI update completed!"
              if [[ "$RESULT" == *"FILE_EXISTS: READY"* ]]; then
                echo "   ‚Üí Detected completion file exists"
              fi
              if [[ "$RESULT" == *"AMI_UPDATE_COMPLETE"* ]]; then
                echo "   ‚Üí Detected completion marker in file content"
              fi
              break
            fi
            
            # Extract current progress for better monitoring
            if [[ "$RESULT" == *"CURRENT_PROGRESS:"* ]]; then
              CURRENT_STEP=$(echo "$RESULT" | grep "CURRENT_PROGRESS:" | cut -d: -f2 | xargs)
              echo "üìç Current step: $CURRENT_STEP"
              
              # Provide context for different steps
              case "$CURRENT_STEP" in
                "UPDATING_EXISTING_AMI")
                  echo "   ‚Üí Updating existing ComfyUI AMI with latest scripts"
                  ;;
                "EXISTING_AMI_UPDATED")
                  echo "   ‚Üí Existing AMI successfully updated"
                  ;;
                "FRESH_SETUP_STARTING")
                  echo "   ‚Üí Setting up ComfyUI on fresh Ubuntu AMI"
                  ;;
                "RUNNING_FULL_SETUP")
                  echo "   ‚Üí Running full AMI preparation (this may take several minutes)"
                  ;;
                "FRESH_SETUP_COMPLETED")
                  echo "   ‚Üí Fresh setup completed successfully"
                  ;;
              esac
            fi
            
            # Check if process is still running
            if [[ "$RESULT" == *"NO_PROCESS_RUNNING"* ]]; then
              echo "‚ö†Ô∏è AMI update process is not running - may have completed or failed"
              echo "üîç Getting detailed logs for debugging..."
            fi
          fi
          
          if [ $i -eq $MAX_ATTEMPTS ]; then
            echo "‚ùå Timeout waiting for AMI update"
            echo "üîç Checking instance logs for debugging..."
            
            # Get last 50 lines of user-data log
            LOG_COMMAND_ID=$(aws ssm send-command \
              --instance-ids $INSTANCE_ID \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=["tail -50 /var/log/user-data.log || echo No user-data log found"]' \
              --region ${{ env.AWS_REGION }} \
              --query 'Command.CommandId' \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$LOG_COMMAND_ID" ]; then
              sleep 10
              aws ssm get-command-invocation \
                --command-id $LOG_COMMAND_ID \
                --instance-id $INSTANCE_ID \
                --region ${{ env.AWS_REGION }} \
                --query 'StandardOutputContent' \
                --output text 2>/dev/null || echo "Could not retrieve logs"
            fi
            
            exit 1
          fi
          
          sleep 15
        done

    - name: Verify ComfyUI setup
      timeout-minutes: 5
      run: |
        INSTANCE_ID="${{ steps.launch-instance.outputs.instance-id }}"
        AMI_TYPE="${{ steps.base-ami.outputs.ami-type }}"
        echo "üîç Verifying ComfyUI setup and health..."
        echo "AMI Type: $AMI_TYPE"
        
        # Check system and tenant manager status
        echo "ÔøΩ Checking system and tenant manager status..."
        
        SYSTEM_CHECK_ID=$(aws ssm send-command \
          --instance-ids $INSTANCE_ID \
          --document-name "AWS-RunShellScript" \
          --parameters 'commands=[
            "echo === ComfyUI System Health Check (Docker-Free) ===",
            "# Check if tenant manager is installed",
            "if [ -f \"/usr/local/bin/tenant_manager.py\" ]; then",
            "  echo TENANT_MANAGER: installed",
            "  ls -la /usr/local/bin/tenant_manager.py",
            "else",
            "  echo TENANT_MANAGER: missing",
            "fi",
            "# Check Python installation",
            "if command -v python3 >/dev/null; then",
            "  echo PYTHON: $(python3 --version)",
            "else",
            "  echo PYTHON: missing",
            "fi",
            "# Check required Python packages",
            "echo === Python Package Check ===",
            "python3 -c \"import psutil; print(f\\\"psutil: {psutil.__version__}\\\")\" 2>/dev/null || echo \"psutil: missing\"",
            "python3 -c \"import boto3; print(f\\\"boto3: {boto3.__version__}\\\")\" 2>/dev/null || echo \"boto3: missing\"",
            "python3 -c \"import requests; print(f\\\"requests: {requests.__version__}\\\")\" 2>/dev/null || echo \"requests: missing\"",
            "# Check systemd service setup",
            "if systemctl list-units --type=service | grep -q comfyui-multitenant; then",
            "  echo SYSTEMD_SERVICE: configured",
            "  systemctl status comfyui-multitenant --no-pager -l || true",
            "else",
            "  echo SYSTEMD_SERVICE: not configured",
            "fi",
            "# Check directories",
            "echo === Directory Structure ===",
            "ls -la /workspace/ || echo \"/workspace not found\"",
            "ls -la /var/log/comfyui/ || echo \"/var/log/comfyui not found\"",
            "ls -la /scripts/ || echo \"/scripts not found\"",
            "# Overall health assessment",
            "if [ -f \"/usr/local/bin/tenant_manager.py\" ] && command -v python3 >/dev/null; then",
            "  echo SYSTEM_HEALTH: HEALTHY",
            "else",
            "  echo SYSTEM_HEALTH: UNHEALTHY",
            "fi"
          ]' \
          --region ${{ env.AWS_REGION }} \
          --query 'Command.CommandId' \
          --output text)
        
        sleep 15
        SYSTEM_STATUS=$(aws ssm get-command-invocation \
          --command-id $SYSTEM_CHECK_ID \
          --instance-id $INSTANCE_ID \
          --region ${{ env.AWS_REGION }} \
          --query 'StandardOutputContent' \
          --output text)
        
        echo "System Status Details:"
        echo "$SYSTEM_STATUS"
        
        # Check for multiple reliable indicators
        TENANT_MANAGER_OK=false
        PYTHON_OK=false
        SYSTEM_HEALTH_OK=false
        
        if [[ "$SYSTEM_STATUS" == *"TENANT_MANAGER: installed"* ]]; then
          TENANT_MANAGER_OK=true
          echo "‚úÖ Tenant manager is installed"
        else
          echo "‚ùå Tenant manager is not installed"
        fi
        
        if [[ "$SYSTEM_STATUS" == *"PYTHON: Python 3"* ]]; then
          PYTHON_OK=true
          echo "‚úÖ Python is installed and working"
        else
          echo "‚ùå Python is not working properly"
        fi
        
        if [[ "$SYSTEM_STATUS" == *"SYSTEM_HEALTH: HEALTHY"* ]]; then
          SYSTEM_HEALTH_OK=true
          echo "‚úÖ Overall system health check passed"
        else
          echo "‚ùå Overall system health check failed"
        fi
        
        # System is considered ready if tenant manager is installed AND Python is working
        if [ "$TENANT_MANAGER_OK" = true ] && [ "$PYTHON_OK" = true ]; then
          echo "üéâ ComfyUI system is ready!"
        else
          echo "‚ùå ComfyUI system is not ready"
          
          # Get additional debugging info
          echo "üîç Getting additional debugging information..."
          DEBUG_ID=$(aws ssm send-command \
            --instance-ids $INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=[
              "echo === User Data Log ===", 
              "tail -50 /var/log/user-data.log 2>/dev/null || echo No user data log found",
              "echo === AMI Preparation Log ===",
              "tail -50 /var/log/ami-preparation.log 2>/dev/null || echo No AMI preparation log found",
              "echo === System Status ===",
              "df -h",
              "free -m",
              "ps aux | grep python",
              "echo === Environment Variables ===",
              "env | grep -E \"PYTHON|COMFY|AMI\" || echo No relevant environment variables found"
            ]' \
            --region ${{ env.AWS_REGION }} \
            --query 'Command.CommandId' \
            --output text)
          
          sleep 10
          DEBUG_INFO=$(aws ssm get-command-invocation \
            --command-id $DEBUG_ID \
            --instance-id $INSTANCE_ID \
            --region ${{ env.AWS_REGION }} \
            --query 'StandardOutputContent' \
            --output text)
          
          echo "Debug Information:"
          echo "$DEBUG_INFO"
          
          exit 1
        fi
        echo "‚úÖ ComfyUI system is ready"
        
        # Verify required Python packages are available
        if [[ "$SYSTEM_STATUS" == *"psutil:"* ]] && [[ "$SYSTEM_STATUS" == *"boto3:"* ]] && [[ "$SYSTEM_STATUS" == *"requests:"* ]]; then
          echo "‚úÖ Required Python packages are installed"
        else
          echo "‚ùå Some required Python packages are missing"
          echo "Package status from health check:"
          echo "$SYSTEM_STATUS" | grep -E "(psutil|boto3|requests):" || echo "No package info in output"
          exit 1
        fi
        
        # For existing AMI, we can skip the functionality test since we know the base works
        # and we just need to verify the scripts were updated
        if [ "$AMI_TYPE" = "existing" ]; then
          echo "‚ö° Skipping functionality test for existing AMI (script update verified)"
          echo "‚úÖ Existing AMI update verification complete"
        else
          echo "üß™ Testing tenant manager functionality for fresh AMI..."
          # Test that we can import the tenant manager without errors
          TEST_ID=$(aws ssm send-command \
            --instance-ids $INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=[
              "echo === Testing Tenant Manager Import ===",
              "cd /usr/local/bin",
              "python3 -c \"import sys; sys.path.append('\'.'\''); import tenant_manager; print('\''Tenant manager import successful'\'')\" 2>&1 || echo \"Import failed\"",
              "echo === Testing Basic Python Functionality ===",
              "python3 -c \"import psutil, boto3, requests; print('\''All required packages imported successfully'\'')\" 2>&1 || echo \"Package import failed\""
            ]' \
            --region ${{ env.AWS_REGION }} \
            --query 'Command.CommandId' \
            --output text)
          
          sleep 10
          TEST_OUTPUT=$(aws ssm get-command-invocation \
            --command-id $TEST_ID \
            --instance-id $INSTANCE_ID \
            --region ${{ env.AWS_REGION }} \
            --query 'StandardOutputContent' \
            --output text)
          
          echo "Test Results:"
          echo "$TEST_OUTPUT"
          
          if [[ "$TEST_OUTPUT" == *"Tenant manager import successful"* ]] && [[ "$TEST_OUTPUT" == *"All required packages imported successfully"* ]]; then
            echo "‚úÖ Fresh AMI functionality test passed"
          else
            echo "‚ùå Fresh AMI functionality test failed"
            exit 1
          fi
          echo "‚úÖ Fresh AMI setup verification complete"
        fi
        
        # Verify system is clean for AMI creation
        echo "üßπ Verifying system is clean for AMI creation..."
        CLEANUP_CHECK_ID=$(aws ssm send-command \
          --instance-ids $INSTANCE_ID \
          --document-name "AWS-RunShellScript" \
          --parameters 'commands=[
            "echo Checking system cleanliness...",
            "# Stop the ComfyUI service if running",
            "systemctl stop comfyui-multitenant.service || true",
            "sleep 5",
            "# Ensure no containers are running",
            "RUNNING_CONTAINERS=$(docker ps -q | wc -l)",
            "echo Running containers: $RUNNING_CONTAINERS",
            "if [ $RUNNING_CONTAINERS -gt 0 ]; then",
            "  echo Stopping running containers for AMI creation...",
            "  docker stop $(docker ps -q) || true",
            "  docker rm $(docker ps -aq) || true",
            "  sleep 3",
            "fi",
            "# Verify Docker images are available",
            "echo === Available Docker Images ===",
            "docker images | head -10",
            "# Check system resources",
            "echo === System Resources ===",
            "echo Memory usage: $(free -h | grep Mem:)",
            "echo Disk usage: $(df -h / | tail -1)",
            "echo === Final Container Check ===",
            "echo Active containers: $(docker ps -q | wc -l)",
            "echo CLEANUP_COMPLETE"
          ]' \
          --region ${{ env.AWS_REGION }} \
          --query 'Command.CommandId' \
          --output text)
        
        sleep 15
        CLEANUP_STATUS=$(aws ssm get-command-invocation \
          --command-id $CLEANUP_CHECK_ID \
          --instance-id $INSTANCE_ID \
          --region ${{ env.AWS_REGION }} \
          --query 'StandardOutputContent' \
          --output text)
        
        echo "System Cleanup Results:"
        echo "$CLEANUP_STATUS"
        
        echo "üéâ All health checks passed! AMI is ready for creation."

    - name: Test network connectivity
      timeout-minutes: 3
      run: |
        INSTANCE_ID="${{ steps.launch-instance.outputs.instance-id }}"
        PUBLIC_IP="${{ steps.launch-instance.outputs.public-ip }}"
        
        echo "üåê Testing network connectivity to instance..."
        echo "Public IP: $PUBLIC_IP"
        
        # Check security group allows outbound traffic
        echo "üîí Checking security group configuration..."
        SG_ID=$(aws ec2 describe-instances \
          --instance-ids $INSTANCE_ID \
          --region ${{ env.AWS_REGION }} \
          --query 'Reservations[0].Instances[0].SecurityGroups[0].GroupId' \
          --output text)
        
        echo "Security Group: $SG_ID"
        
        # Get security group rules
        aws ec2 describe-security-groups \
          --group-ids $SG_ID \
          --region ${{ env.AWS_REGION }} \
          --query 'SecurityGroups[0].IpPermissions[*].[IpProtocol,FromPort,ToPort,IpRanges[0].CidrIp]' \
          --output table || true
        
        # Test basic connectivity from GitHub Actions runner
        echo "üèì Testing ICMP ping from GitHub Actions runner..."
        if ping -c 3 -W 5 $PUBLIC_IP; then
          echo "‚úÖ Ping successful from GitHub Actions"
        else
          echo "‚ö†Ô∏è Ping failed from GitHub Actions (this might be normal due to security groups)"
        fi
        
        # Test if we can reach common ports (this will fail but shows if security group is too restrictive)
        echo "üîå Testing port connectivity..."
        for port in 22 80 8188; do
          if timeout 5 nc -z $PUBLIC_IP $port; then
            echo "‚úÖ Port $port is accessible"
          else
            echo "‚ùå Port $port is not accessible (expected for AMI building)"
          fi
        done
        
        echo "‚ÑπÔ∏è Note: For security, the instance should NOT be accessible from the internet during AMI creation"
        echo "‚ÑπÔ∏è When instances are launched from the AMI, proper security groups should be applied"

    - name: Create or update AMI
      id: create-ami
      timeout-minutes: 10
      run: |
        INSTANCE_ID="${{ steps.launch-instance.outputs.instance-id }}"
        ENVIRONMENT="${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}"
        AMI_TYPE="${{ steps.base-ami.outputs.ami-type }}"
        
        # Use consistent AMI naming (no timestamp for updates)
        AMI_NAME="comfyui-multitenant-${ENVIRONMENT}"
        
        echo "üì∏ Creating/updating AMI from instance: $INSTANCE_ID"
        echo "üè∑Ô∏è AMI name: $AMI_NAME"
        echo "üîÑ AMI type: $AMI_TYPE"
        
        # Validate that the system is ready for AMI creation
        echo "‚úÖ System validated and ready for AMI creation"
        
        # If this was an update to an existing AMI, we need to deregister the old one first
        # Also check for any existing AMI with the same name (regardless of AMI_TYPE)
        EXISTING_AMI_WITH_NAME=$(aws ec2 describe-images \
          --owners self \
          --filters "Name=name,Values=$AMI_NAME" \
          --query 'Images[0].ImageId' \
          --output text \
          --region ${{ env.AWS_REGION }} 2>/dev/null || echo "None")
        
        if [ "$AMI_TYPE" = "existing" ]; then
          OLD_AMI_ID="${{ steps.base-ami.outputs.ami-id }}"
          echo "üóëÔ∏è Preparing to replace existing AMI: $OLD_AMI_ID"
        elif [ -n "$EXISTING_AMI_WITH_NAME" ] && [ "$EXISTING_AMI_WITH_NAME" != "None" ]; then
          OLD_AMI_ID="$EXISTING_AMI_WITH_NAME"
          echo "üóëÔ∏è Found existing AMI with same name, will replace: $OLD_AMI_ID"
        else
          OLD_AMI_ID=""
          echo "üÜï Creating fresh AMI (no existing AMI found)"
        fi
        
        # Get snapshots for cleanup if we have an old AMI to replace
        if [ -n "$OLD_AMI_ID" ] && [ "$OLD_AMI_ID" != "None" ]; then
          OLD_SNAPSHOTS=$(aws ec2 describe-images \
            --image-ids $OLD_AMI_ID \
            --query 'Images[0].BlockDeviceMappings[*].Ebs.SnapshotId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")
          
          echo "üìã Old AMI to replace: $OLD_AMI_ID"
          echo "üìã Old AMI snapshots to clean up: $OLD_SNAPSHOTS"
          
          # Deregister the old AMI BEFORE creating the new one to avoid name conflict
          echo "üóëÔ∏è Deregistering old AMI to avoid name conflict..."
          aws ec2 deregister-image --image-id $OLD_AMI_ID --region ${{ env.AWS_REGION }} || {
            echo "‚ö†Ô∏è Failed to deregister old AMI, continuing anyway..."
          }
          
          # Wait a moment for deregistration to propagate
          sleep 5
        else
          OLD_SNAPSHOTS=""
        fi
        
        # Stop the instance before creating AMI
        aws ec2 stop-instances --instance-ids $INSTANCE_ID --region ${{ env.AWS_REGION }}
        aws ec2 wait instance-stopped --instance-ids $INSTANCE_ID --region ${{ env.AWS_REGION }}
        
        # Create new AMI with consistent naming
        NEW_AMI_ID=$(aws ec2 create-image \
          --instance-id $INSTANCE_ID \
          --name "$AMI_NAME" \
          --description "ComfyUI Multi-Tenant AMI for $ENVIRONMENT environment (Docker-Free) - updated $(date +%Y-%m-%d)" \
          --region ${{ env.AWS_REGION }} \
          --tag-specifications 'ResourceType=image,Tags=[{Key=Name,Value='"$AMI_NAME"'},{Key=Environment,Value='"$ENVIRONMENT"'},{Key=DeploymentType,Value="Docker-Free"},{Key=Branch,Value=${{ github.ref_name }}},{Key=UpdateType,Value='"$AMI_TYPE"'},{Key=LastUpdated,Value='$(date -u +"%Y-%m-%dT%H:%M:%SZ")'}]' \
          --query 'ImageId' \
          --output text)
        
        echo "ami-id=$NEW_AMI_ID" >> $GITHUB_OUTPUT
        echo "old-ami-id=${OLD_AMI_ID:-}" >> $GITHUB_OUTPUT
        echo "old-snapshots=${OLD_SNAPSHOTS:-}" >> $GITHUB_OUTPUT
        echo "‚úÖ New AMI created: $NEW_AMI_ID"
        
        if [ -n "$OLD_AMI_ID" ] && [ "$OLD_AMI_ID" != "None" ]; then
          echo "üîÑ This replaces the previous AMI: $OLD_AMI_ID"
        else
          echo "üÜï This is a fresh AMI creation"
        fi

    - name: Wait for AMI to be available
      timeout-minutes: 15
      run: |
        AMI_ID="${{ steps.create-ami.outputs.ami-id }}"
        echo "‚è≥ Waiting for AMI to be available..."
        aws ec2 wait image-available --image-ids $AMI_ID --region ${{ env.AWS_REGION }}
        echo "‚úÖ AMI is available: $AMI_ID"

    - name: Clean up old AMI snapshots
      if: steps.create-ami.outputs.old-snapshots != ''
      run: |
        OLD_SNAPSHOTS="${{ steps.create-ami.outputs.old-snapshots }}"
        NEW_AMI_ID="${{ steps.create-ami.outputs.ami-id }}"
        OLD_AMI_ID="${{ steps.create-ami.outputs.old-ami-id }}"
        
        echo "üßπ Cleaning up old AMI snapshots now that new AMI is available..."
        echo "New AMI: $NEW_AMI_ID"
        echo "Old AMI (already deregistered): $OLD_AMI_ID"
        
        # Clean up old snapshots (AMI was already deregistered during creation)
        if [ -n "$OLD_SNAPSHOTS" ] && [ "$OLD_SNAPSHOTS" != "None" ] && [ "$OLD_SNAPSHOTS" != "" ]; then
          echo "üóëÔ∏è Cleaning up old snapshots: $OLD_SNAPSHOTS"
          for SNAPSHOT in $OLD_SNAPSHOTS; do
            if [ "$SNAPSHOT" != "None" ] && [ -n "$SNAPSHOT" ]; then
              echo "üóëÔ∏è Deleting snapshot: $SNAPSHOT"
              aws ec2 delete-snapshot --snapshot-id $SNAPSHOT --region ${{ env.AWS_REGION }} || echo "‚ö†Ô∏è Failed to delete snapshot $SNAPSHOT"
            fi
          done
          
          echo "‚úÖ Old AMI snapshots cleaned up successfully"
        else
          echo "‚ÑπÔ∏è No old snapshots to clean up"
        fi

    - name: Store AMI details in SSM Parameter Store
      run: |
        ENVIRONMENT="${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}"
        AMI_ID="${{ steps.create-ami.outputs.ami-id }}"
        CREATION_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
        BRANCH_NAME="${{ github.ref_name }}"
        COMMIT_SHA="${{ github.sha }}"
        
        echo "üìù Storing AMI details in SSM Parameter Store..."
        echo "Environment: $ENVIRONMENT"
        echo "AMI ID: $AMI_ID"
        echo "Docker Image: $DOCKER_IMAGE"
        
        # Store the latest AMI ID
        aws ssm put-parameter \
          --name "/comfyui/ami/$ENVIRONMENT/latest" \
          --value "$AMI_ID" \
          --type "String" \
          --overwrite \
          --description "Latest ComfyUI AMI ID for $ENVIRONMENT environment" \
          --region ${{ env.AWS_REGION }}
        
        echo "‚úÖ Stored AMI ID in /comfyui/ami/$ENVIRONMENT/latest"
        
        # Store detailed metadata as JSON
        METADATA=$(cat << EOF
        {
          "ami_id": "$AMI_ID",
          "docker_image": "$DOCKER_IMAGE",
          "environment": "$ENVIRONMENT",
          "creation_date": "$CREATION_DATE",
          "branch": "$BRANCH_NAME",
          "commit_sha": "$COMMIT_SHA",
          "region": "${{ env.AWS_REGION }}",
          "workflow_run_id": "${{ github.run_id }}",
          "workflow_run_number": "${{ github.run_number }}"
        }
        EOF
        )
        
        aws ssm put-parameter \
          --name "/comfyui/ami/$ENVIRONMENT/metadata" \
          --value "$METADATA" \
          --type "String" \
          --overwrite \
          --description "Detailed metadata for latest ComfyUI AMI in $ENVIRONMENT environment" \
          --region ${{ env.AWS_REGION }}
        
        echo "‚úÖ Stored AMI metadata in /comfyui/ami/$ENVIRONMENT/metadata"
        
        # Also store a timestamped version for history
        aws ssm put-parameter \
          --name "/comfyui/ami/$ENVIRONMENT/history/$AMI_ID" \
          --value "$METADATA" \
          --type "String" \
          --description "Historical metadata for AMI $AMI_ID in $ENVIRONMENT environment" \
          --region ${{ env.AWS_REGION }}
        
        echo "‚úÖ Stored historical AMI metadata in /comfyui/ami/$ENVIRONMENT/history/$AMI_ID"
        
        echo ""
        echo "üìã SSM Parameter Store Summary:"
        echo "   Latest AMI ID: /comfyui/ami/$ENVIRONMENT/latest"
        echo "   Latest Metadata: /comfyui/ami/$ENVIRONMENT/metadata"
        echo "   Historical Record: /comfyui/ami/$ENVIRONMENT/history/$AMI_ID"
        echo ""
        echo "üí° Access in your code using AWS SDK:"
        echo "   aws ssm get-parameter --name '/comfyui/ami/$ENVIRONMENT/latest' --region ${{ env.AWS_REGION }}"

    - name: Cleanup resources
      if: always()
      run: |
        INSTANCE_ID="${{ steps.launch-instance.outputs.instance-id }}"
        
        if [ -n "$INSTANCE_ID" ]; then
          echo "üßπ Cleaning up instance: $INSTANCE_ID"
          aws ec2 terminate-instances --instance-ids $INSTANCE_ID --region ${{ env.AWS_REGION }} || echo "‚ö†Ô∏è Failed to terminate instance"
          echo "‚úÖ Instance termination initiated"
        else
          echo "‚ÑπÔ∏è No instance to cleanup"
        fi

    - name: Display deployment summary
      run: |
        ENVIRONMENT="${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}"
        AMI_TYPE="${{ steps.base-ami.outputs.ami-type }}"
        OLD_AMI="${{ steps.create-ami.outputs.old-ami-id }}"
        
        echo ""
        echo "üéâ Deployment Summary"
        echo "===================="
        echo "Environment: $ENVIRONMENT"
        echo "AMI ID: ${{ steps.create-ami.outputs.ami-id }}"
        echo "Instance ID: ${{ steps.launch-instance.outputs.instance-id }} (terminated)"
        echo "S3 Scripts: s3://viral-comm-api-ec2-deployments-dev/comfy-docker/$ENVIRONMENT/"
        echo ""
        echo "üì¶ AMI Management:"
        if [ "$AMI_TYPE" = "existing" ]; then
          echo "  üîÑ Updated existing AMI with latest scripts"
          if [ -n "$OLD_AMI" ]; then
            echo "  üóëÔ∏è Replaced old AMI: $OLD_AMI"
          fi
          echo "  ‚ö° Faster deployment using incremental updates"
        else
          echo "  üÜï Created fresh AMI from Ubuntu base"
          echo "  üìã First-time setup completed"
        fi
        echo ""
        echo "üîó SSM Parameter Store:"
        echo "  üìç Latest AMI: /comfyui/ami/$ENVIRONMENT/latest"
        echo "  üìä Metadata: /comfyui/ami/$ENVIRONMENT/metadata"
        echo "  üìö History: /comfyui/ami/$ENVIRONMENT/history/${{ steps.create-ami.outputs.ami-id }}"
        echo ""
        echo "‚úÖ AMI is ready for deployment!"
        echo ""
        echo "üöÄ Next Steps:"
        echo "1. Retrieve AMI ID: aws ssm get-parameter --name '/comfyui/ami/$ENVIRONMENT/latest'"
        echo "2. Use the AMI ID to launch instances"
        echo "3. ComfyUI will start automatically on port 8188"
        echo "4. Custom nodes and models are pre-installed"
        echo "5. Instance will auto-configure on first boot"
        echo ""
        echo "üí° Optimization Benefits:"
        if [ "$AMI_TYPE" = "existing" ]; then
          echo "  ‚Ä¢ Faster builds using existing AMI base"
          echo "  ‚Ä¢ Only Docker image gets updated"
          echo "  ‚Ä¢ Consistent environment across updates"
        else
          echo "  ‚Ä¢ Fresh environment with latest Ubuntu base"
          echo "  ‚Ä¢ Full system setup completed"
          echo "  ‚Ä¢ Future updates will be incremental"
        fi